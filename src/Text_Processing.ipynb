{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5880fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Keerthan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",

    "import nltk\n",
    "nltk.download('words')\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 2,

   "id": "81a898e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "zf = zipfile.ZipFile(\"../data/merged_finance.zip\")\n",
    "df = pd.read_csv(zf.open('merged_finance.csv'))"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 3,

   "id": "462bc4b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             DATE                                            ARTICLE  \\\n",
      "0      2005-12-09  ['Guy Quaden: The National Bank of Belgium - a...   \n",
      "1      2009-12-07  ['Guy Quaden: A changing IMF and World Bank\\n'...   \n",
      "2      2018-12-20  ['1.\\n', '\\n', 'Central Banks and money: an ev...   \n",
      "3      2002-05-28  ['Guy Quaden: The Euro - a milestone on the pa...   \n",
      "4      2002-05-28  ['Guy Quaden: The Euro - a milestone on the pa...   \n",
      "...           ...                                                ...   \n",
      "17023  2013-08-27  ['Zeti Akhtar Aziz: Grow your business – acces...   \n",
      "17024  2010-01-28  ['Mohd Razif bin Abd Kadir: Islamic finance an...   \n",
      "17025  2017-05-26  ['Encik Abdul Rasheed Ghaffour: Revolutionisin...   \n",
      "17026  2014-05-07  ['Zeti Akhtar Aziz: Nurturing young talent in ...   \n",
      "17027  2005-09-23  ['Datuk Zamani Abdul Ghani: Role of developmen...   \n",
      "\n",
      "       Diff_VIX_1d  Diff_VIX_1w  Diff_VIX_2w   OPEN   HIGH    LOW  CLOSE  \n",
      "0            11.69          NaN          NaN  11.91  12.20  11.56  11.69  \n",
      "1            22.10          NaN          NaN  22.32  22.46  21.60  22.10  \n",
      "2            28.38          NaN          NaN  26.03  30.30  25.35  28.38  \n",
      "3            20.31          NaN          NaN  20.21  20.77  20.07  20.31  \n",
      "4            20.31          NaN          NaN  20.21  20.77  20.07  20.31  \n",
      "...            ...          ...          ...    ...    ...    ...    ...  \n",
      "17023        16.77          NaN          NaN  16.56  17.13  15.82  16.77  \n",
      "17024        23.73          NaN          NaN  22.79  25.30  22.69  23.73  \n",
      "17025         9.81          NaN          NaN   9.93  10.48   9.65   9.81  \n",
      "17026        13.40          NaN          NaN  13.64  14.49  13.39  13.40  \n",
      "17027        12.96          NaN          NaN  13.64  13.88  12.75  12.96  \n",
      "\n",
      "[17028 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2939c20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ARTICLE</th>\n",
       "      <th>Diff_VIX_1d</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['Guy Quaden: The National Bank of Belgium - a...</td>\n",
       "      <td>11.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['Guy Quaden: A changing IMF and World Bank\\n'...</td>\n",
       "      <td>22.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['1.\\n', '\\n', 'Central Banks and money: an ev...</td>\n",
       "      <td>28.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['Guy Quaden: The Euro - a milestone on the pa...</td>\n",
       "      <td>20.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['Guy Quaden: The Euro - a milestone on the pa...</td>\n",
       "      <td>20.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17023</th>\n",
       "      <td>['Zeti Akhtar Aziz: Grow your business – acces...</td>\n",
       "      <td>16.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17024</th>\n",
       "      <td>['Mohd Razif bin Abd Kadir: Islamic finance an...</td>\n",
       "      <td>23.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17025</th>\n",
       "      <td>['Encik Abdul Rasheed Ghaffour: Revolutionisin...</td>\n",
       "      <td>9.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17026</th>\n",
       "      <td>['Zeti Akhtar Aziz: Nurturing young talent in ...</td>\n",
       "      <td>13.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17027</th>\n",
       "      <td>['Datuk Zamani Abdul Ghani: Role of developmen...</td>\n",
       "      <td>12.96</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17028 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 ARTICLE  Diff_VIX_1d\n",
       "0      ['Guy Quaden: The National Bank of Belgium - a...        11.69\n",
       "1      ['Guy Quaden: A changing IMF and World Bank\\n'...        22.10\n",
       "2      ['1.\\n', '\\n', 'Central Banks and money: an ev...        28.38\n",
       "3      ['Guy Quaden: The Euro - a milestone on the pa...        20.31\n",
       "4      ['Guy Quaden: The Euro - a milestone on the pa...        20.31\n",
       "...                                                  ...          ...\n",
       "17023  ['Zeti Akhtar Aziz: Grow your business – acces...        16.77\n",
       "17024  ['Mohd Razif bin Abd Kadir: Islamic finance an...        23.73\n",
       "17025  ['Encik Abdul Rasheed Ghaffour: Revolutionisin...         9.81\n",
       "17026  ['Zeti Akhtar Aziz: Nurturing young talent in ...        13.40\n",
       "17027  ['Datuk Zamani Abdul Ghani: Role of developmen...        12.96\n",
       "\n",
       "[17028 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(columns = [\"DATE\", \"Diff_VIX_1w\", \"Diff_VIX_2w\", \"OPEN\", \"HIGH\", \"LOW\", \"CLOSE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f01656a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={\"Diff_VIX_1d\": \"VIX_1day\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0394bf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"ARTICLE\"].replace(to_replace=[r\"\\\\t|\\\\n|\\\\r\", \"\\t|\\n|\\r\"], value=[\"\",\"\"], regex=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "96b0b593",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(x):\n",
    "\n",
    "    x = str(x)\n",
    "    for punct in \"/-'\":\n",
    "        x = x.replace(punct, ' ')\n",
    "    for punct in '&':\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "    for punct in 'â€–?!.,â€™\"#$%Ã©\\'()*+-/:;<=>@[\\\\]^_`{|}~' + '“”’':\n",
    "        x = x.replace(punct, '')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56182143",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [

      "100%|██████████████████████████████████████████████████████████████████████████| 17028/17028 [00:06<00:00, 2623.77it/s]\n"
     ]
    }
   ],
   "source": [
    "df[\"ARTICLE\"] = df[\"ARTICLE\"].progress_apply(lambda x: clean_text(x))\n",
    "sentences = df[\"ARTICLE\"].apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c7a6f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define regular expressions for numbers and links\n",
    "num_regex = r'\\d+' # matches one or more digits\n",
    "link_regex = r'https?://\\S+' # matches http or https followed by any non-space characters\n",
    "\n",
    "# remove numbers and links from the text column\n",
    "df['ARTICLE'] = df['ARTICLE'].apply(lambda x: re.sub(num_regex, '', x)) \n",
    "df['ARTICLE'] = df['ARTICLE'].apply(lambda x: re.sub(link_regex, '', x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16c76710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URLs have been successfully removed\n",
      "0         Guy Quaden The National Bank of Belgium   a c...\n",
      "1         Guy Quaden A changing IMF and World Bank   Sp...\n",
      "2               Central Banks and money an everchanging...\n",
      "3         Guy Quaden The Euro   a milestone on the path...\n",
      "4         Guy Quaden The Euro   a milestone on the path...\n",
      "                               ...                        \n",
      "17023     Zeti Akhtar Aziz Grow your business  access t...\n",
      "17024     Mohd Razif bin Abd Kadir Islamic finance and ...\n",
      "17025     Encik Abdul Rasheed Ghaffour Revolutionising ...\n",
      "17026     Zeti Akhtar Aziz Nurturing young talent in Ma...\n",
      "17027     Datuk Zamani Abdul Ghani Role of development ...\n",
      "Name: ARTICLE, Length: 17028, dtype: object\n"
     ]
    }
   ],
   "source": [
    "if 'http' not in df['ARTICLE'].values and 'https' not in df['ARTICLE'].values:\n",
    "    print('URLs have been successfully removed')\n",
    "else:\n",
    "    print('URLs have not been removed')\n",
    "print(df[\"ARTICLE\"])"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 10,
   "id": "12454f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def pre_process(text):\n",
    "    \n",
    "    # lowercase\n",
    "    text=text.lower()\n",
    "    #remove tags\n",
    "    text=re.sub(\"</?.*?>\",\" <> \",text)\n",
    "    \n",
    "    # remove special characters and digits\n",
    "    text = re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "    # remove all url links\n",
    "    text = re.sub(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))''', \" \", text)\n",
    "    text = re.sub('http://\\S+|https://\\S+', '', text)\n",
    "    text = re.sub('http[s]?://\\S+', '', text)\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe3eb1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"ARTICLE\"] = [pre_process(i) for i in df[\"ARTICLE\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5253437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         guy quaden the national bank of belgium a cen...\n",
      "1         guy quaden a changing imf and world bank spee...\n",
      "2         central banks and money an everchanging inter...\n",
      "3         guy quaden the euro a milestone on the path o...\n",
      "4         guy quaden the euro a milestone on the path o...\n",
      "                               ...                        \n",
      "17023     zeti akhtar aziz grow your business access to...\n",
      "17024     mohd razif bin abd kadir islamic finance and ...\n",
      "17025     encik abdul rasheed ghaffour revolutionising ...\n",
      "17026     zeti akhtar aziz nurturing young talent in ma...\n",
      "17027     datuk zamani abdul ghani role of development ...\n",
      "Name: ARTICLE, Length: 17028, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df[\"ARTICLE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23befa6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = set(nltk.corpus.words.words())\n",
    "\n",
    "def clean_sent(sent):\n",
    "    return \" \".join(w for w in nltk.wordpunct_tokenize(sent) \\\n",
    "     if w.lower() in words or not w.isalpha())\n",
    "\n",
    "df['ARTICLE'] = df['ARTICLE'].apply(clean_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,

   "id": "be5eb49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(sentences, verbose =  True):\n",
    "    \"\"\"\n",
    "    :param sentences: list of list of words\n",
    "    :return: dictionary of words and their count\n",
    "    \"\"\"\n",
    "    vocab = {}\n",
    "    for sentence in tqdm(sentences, disable = (not verbose)):\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 15,

   "id": "71b583c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [

      "100%|██████████████████████████████████████████████████████████████████████████| 17028/17028 [00:08<00:00, 1959.43it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 17028/17028 [00:10<00:00, 1573.29it/s]\n"

     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [

      "{'guy': 278, 'the': 3440085, 'national': 36097, 'bank': 187831, 'of': 1801753}\n"

     ]
    }
   ],
   "source": [
    "sentences = df[\"ARTICLE\"].progress_apply(lambda x: x.split()).values\n",
    "vocab = build_vocab(sentences)\n",
    "print({k: vocab[k] for k in list(vocab)[:5]})"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 16,

   "id": "9e6ddc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",

    "news_path = '../../Embeddings/GoogleNews-vectors-negative300.bin'\n",

    "embeddings_index = KeyedVectors.load_word2vec_format(news_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 17,

   "id": "af1cae8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "def check_coverage(vocab,embeddings_index):\n",
    "    a = {}\n",
    "    oov = {}\n",
    "    k = 0\n",
    "    i = 0\n",
    "    for word in tqdm(vocab):\n",
    "        try:\n",
    "            a[word] = embeddings_index[word]\n",
    "            k += vocab[word]\n",
    "        except:\n",
    "\n",
    "            oov[word] = vocab[word]\n",
    "            i += vocab[word]\n",
    "            pass\n",
    "\n",
    "    print('Found embeddings for {:.2%} of vocab'.format(len(a) / len(vocab)))\n",
    "    print('Found embeddings for  {:.2%} of all text'.format(k / (k + i)))\n",
    "    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n",
    "    \n",
    "    return sorted_x"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 18,

   "id": "eb587fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [

      "100%|██████████████████████████████████████████████████████████████████████████| 25773/25773 [00:04<00:00, 6034.35it/s]"

     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [

      "Found embeddings for 94.27% of vocab\n",
      "Found embeddings for  86.32% of all text\n"

     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "oov = check_coverage(vocab,embeddings_index)"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 19,

   "id": "c2f9d0d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [

       "[('of', 1801753),\n",
       " ('to', 1442017),\n",
       " ('and', 1405911),\n",
       " ('a', 828324),\n",
       " ('labour', 20570),\n",
       " ('behaviour', 5956),\n",
       " ('covid', 2996),\n",
       " ('analyses', 2329),\n",
       " ('analyse', 1064),\n",
       " ('doesnt', 1008)]"
      ]
     },
     "execution_count": 19,

     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top 10 out of vocabulary words with their frequency\n",
    "oov[:10]"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 20,

   "id": "1b5e6c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [

      "100%|███████████████████████████████████████████████████████████████████████████| 17028/17028 [00:25<00:00, 671.82it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 17028/17028 [00:08<00:00, 1996.86it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 17028/17028 [00:08<00:00, 1988.04it/s]\n"

     ]
    }
   ],
   "source": [
    "sentences = df[\"ARTICLE\"].progress_apply(lambda x: x.split())\n",
    "to_remove = ['a','to','of','and']\n",
    "sentences = [[word for word in sentence if not word in to_remove] for sentence in tqdm(sentences)]\n",
    "vocab = build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 21,

   "id": "fb46b870",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [

      "100%|█████████████████████████████████████████████████████████████████████████| 25769/25769 [00:00<00:00, 35977.32it/s]"

     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [

      "Found embeddings for 94.29% of vocab\n",
      "Found embeddings for  99.86% of all text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"

     ]
    }
   ],
   "source": [
    "oov = check_coverage(vocab,embeddings_index)"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 22,

   "id": "be55b26f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [

       "[('labour', 20570),\n",
       " ('behaviour', 5956),\n",
       " ('covid', 2996),\n",
       " ('analyses', 2329),\n",
       " ('analyse', 1064),\n",
       " ('doesnt', 1008),\n",
       " ('defence', 959),\n",
       " ('learnt', 918),\n",
       " ('cheque', 622),\n",
       " ('didnt', 599),\n",
       " ('macao', 555),\n",
       " ('mimeo', 442),\n",
       " ('channelled', 348),\n",
       " ('shouldnt', 347),\n",
       " ('wasnt', 291),\n",
       " ('channelling', 256),\n",
       " ('sabine', 251),\n",
       " ('elb', 232),\n",
       " ('resolvability', 232),\n",
       " ('kiley', 224)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "97916db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_mispell(mispell_dict):\n",
    "    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
    "    return mispell_dict, mispell_re\n",
    "\n",
    "mispell_dict = {'colour':'color',\n",
    "                'centre':'center',\n",
    "                'didnt':'did not',\n",
    "                'doesnt':'does not',\n",
    "                'isnt':'is not',\n",
    "                'harbour': 'harbor',\n",
    "                'flavour': 'flavor',\n",
    "                'wasnt': 'was not',\n",
    "                'labour': 'labor',\n",
    "                'learnt':'learn',\n",
    "                'defence': 'defense',\n",
    "                'analyses': 'analysis',\n",
    "                'behaviour': 'behavior',\n",
    "                'travelled': 'traveled',\n",
    "                'channelled': 'channeled',\n",
    "                'channelling': 'channeling',\n",
    "                'labour': 'labor',\n",
    "                'analyse': 'analize',\n",
    "                'shouldnt':'should not',\n",
    "                'favourite':'favorite',\n",
    "                'travelling':'traveling',\n",
    "                'counselling':'counseling',\n",
    "                'theatre':'theater',\n",
    "                'cancelled':'canceled',\n",
    "                'labour':'labor',\n",
    "                'organisation':'organization',\n",
    "                'wwii':'world war 2',\n",
    "                'citicise':'criticize',\n",
    "                'instagram': 'social medium',\n",
    "                'whatsapp': 'social medium',\n",
    "                'snapchat': 'social medium'\n",
    "               }\n",
    "\n",
    "mispellings, mispellings_re = _get_mispell(mispell_dict)\n",
    "\n",
    "def replace_typical_misspell(text):\n",
    "    def replace(match):\n",
    "        return mispellings[match.group(0)]\n",
    "\n",
    "    return mispellings_re.sub(replace, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a010467c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 17028/17028 [00:27<00:00, 618.98it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 17028/17028 [00:06<00:00, 2582.26it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 17028/17028 [00:09<00:00, 1724.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'guy': 278, 'the': 3440085, 'national': 36097, 'bank': 187831, 'of': 1801753}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df[\"ARTICLE\"] = df[\"ARTICLE\"].progress_apply(lambda x: replace_typical_misspell(x))\n",
    "sentences = df[\"ARTICLE\"].progress_apply(lambda x: x.split()).values\n",
    "vocab = build_vocab(sentences)\n",
    "print({k: vocab[k] for k in list(vocab)[:5]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "04bfda9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 25757/25757 [00:01<00:00, 17189.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 94.33% of vocab\n",
      "Found embeddings for  86.41% of all text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "oov = check_coverage(vocab,embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e727710a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('of', 1801753),\n",
       " ('to', 1442017),\n",
       " ('and', 1405911),\n",
       " ('a', 828324),\n",
       " ('covid', 2996),\n",
       " ('cheque', 622),\n",
       " ('macao', 555),\n",
       " ('mimeo', 442),\n",
       " ('sabine', 251),\n",
       " ('elb', 232),\n",
       " ('resolvability', 232),\n",
       " ('kiley', 224),\n",
       " ('grey', 173),\n",
       " ('exter', 143),\n",
       " ('aluminium', 131),\n",
       " ('rix', 125),\n",
       " ('berne', 124),\n",
       " ('saron', 123),\n",
       " ('paolo', 116),\n",
       " ('pank', 108)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "107e6cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 17028/17028 [00:23<00:00, 734.45it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 17028/17028 [00:04<00:00, 3531.03it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 17028/17028 [00:06<00:00, 2588.63it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 17028/17028 [00:07<00:00, 2141.40it/s]\n"
     ]
    }
   ],
   "source": [
    "df[\"ARTICLE\"] = df[\"ARTICLE\"].progress_apply(lambda x: replace_typical_misspell(x))\n",
    "sentences = df[\"ARTICLE\"].progress_apply(lambda x: x.split())\n",
    "to_remove = ['a','to','of','and']\n",
    "sentences = [[word for word in sentence if not word in to_remove] for sentence in tqdm(sentences)]\n",
    "vocab = build_vocab(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e9c5ac70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 25753/25753 [00:01<00:00, 24891.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 94.35% of vocab\n",
      "Found embeddings for  99.96% of all text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "oov = check_coverage(vocab,embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "934ea4f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('covid', 2996),\n",
       " ('cheque', 622),\n",
       " ('macao', 555),\n",
       " ('mimeo', 442),\n",
       " ('sabine', 251),\n",
       " ('elb', 232),\n",
       " ('resolvability', 232),\n",
       " ('kiley', 224),\n",
       " ('grey', 173),\n",
       " ('exter', 143),\n",
       " ('aluminium', 131),\n",
       " ('rix', 125),\n",
       " ('berne', 124),\n",
       " ('saron', 123),\n",
       " ('paolo', 116),\n",
       " ('pank', 108),\n",
       " ('enquiry', 99),\n",
       " ('whiteside', 98),\n",
       " ('faust', 93),\n",
       " ('schnabel', 86)]"
      ]
     },
     "execution_count": 29,

     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top 20 out of vocabulary words with their frequency\n",
    "oov[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f7acfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
